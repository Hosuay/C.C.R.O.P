{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3gSmk3oPOIP",
        "outputId": "58c99212-0d74-4858-f5e9-b81f3ec11064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n",
            "‚úì Found kaggle.json in current directory\n",
            "‚úì Kaggle credentials installed to /root/.kaggle/kaggle.json\n",
            "\n",
            "Verifying Kaggle API access...\n",
            "‚úì Kaggle API authenticated successfully\n",
            "üì• Downloading dataset: engineeringubu/leaf-manifestation-diseases-of-cannabis\n",
            "‚ö†Ô∏è  Primary dataset failed. Trying fallback: vipoooool/new-plant-diseases-dataset\n",
            "‚úì Dataset downloaded successfully\n",
            "üì¶ Extracting: new-plant-diseases-dataset.zip\n",
            "‚úì Dataset extracted successfully\n",
            "‚úì Dataset path: ./dataset\n",
            "‚úì Classes detected: 3\n",
            "  Sample classes: ['New Plant Diseases Dataset(Augmented)', 'new plant diseases dataset(augmented)', 'test']\n",
            "‚úì Dataset split: Train=140613, Val=17576, Test=17578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 162MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Model: resnet18 on cpu\n",
            "  Total parameters: 11,178,051\n",
            "  Trainable parameters: 11,178,051\n",
            "\n",
            "============================================================\n",
            "üöÄ Starting Training\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================\n",
        "# CCROP Cannabis Leaf Disease AI - Enhanced Professional Version\n",
        "# ==============================\n",
        "\n",
        "# 1. Install dependencies\n",
        "!apt-get install -y unzip\n",
        "!pip install torch torchvision torchaudio matplotlib pandas scikit-learn opencv-python kaggle seaborn pillow --quiet\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Import libraries\n",
        "# ------------------------------\n",
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# ------------------------------\n",
        "# 3. Configuration Class\n",
        "# ------------------------------\n",
        "class CCROPConfig:\n",
        "    \"\"\"Centralized configuration for CCROP pipeline\"\"\"\n",
        "\n",
        "    # Kaggle credentials (optional - will use uploaded kaggle.json if available)\n",
        "    KAGGLE_USERNAME = None  # Leave as None to use kaggle.json\n",
        "    KAGGLE_KEY = None  # Leave as None to use kaggle.json\n",
        "\n",
        "    # Dataset settings\n",
        "    DATASET_MAIN = \"engineeringubu/leaf-manifestation-diseases-of-cannabis\"\n",
        "    DATASET_FALLBACK = \"vipoooool/new-plant-diseases-dataset\"\n",
        "    ROOT_DIR = \"./dataset\"\n",
        "\n",
        "    # Model settings\n",
        "    MODEL_ARCH = \"resnet18\"  # Options: resnet18, resnet34, resnet50, efficientnet_b0\n",
        "    INPUT_SIZE = 224\n",
        "    NUM_WORKERS = 2\n",
        "\n",
        "    # Training hyperparameters\n",
        "    BATCH_SIZE = 16\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "    EPOCHS = 15\n",
        "    EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "    # Data split\n",
        "    TRAIN_SPLIT = 0.8\n",
        "    VAL_SPLIT = 0.1\n",
        "    TEST_SPLIT = 0.1\n",
        "\n",
        "    # Paths\n",
        "    CHECKPOINT_DIR = \"./checkpoints\"\n",
        "    LOGS_DIR = \"./logs\"\n",
        "    RESULTS_DIR = \"./results\"\n",
        "\n",
        "    @classmethod\n",
        "    def setup_directories(cls):\n",
        "        \"\"\"Create necessary directories\"\"\"\n",
        "        for dir_path in [cls.ROOT_DIR, cls.CHECKPOINT_DIR, cls.LOGS_DIR, cls.RESULTS_DIR]:\n",
        "            os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# Initialize configuration\n",
        "config = CCROPConfig()\n",
        "config.setup_directories()\n",
        "\n",
        "# ------------------------------\n",
        "# 4. Kaggle Authentication with kaggle.json Support\n",
        "# ------------------------------\n",
        "def setup_kaggle_credentials(config):\n",
        "    \"\"\"Setup Kaggle credentials from kaggle.json or config\"\"\"\n",
        "\n",
        "    # Check for uploaded kaggle.json file first\n",
        "    kaggle_json_path = Path(\"kaggle.json\")\n",
        "    kaggle_dir = Path.home() / \".kaggle\"\n",
        "    kaggle_config_path = kaggle_dir / \"kaggle.json\"\n",
        "\n",
        "    # Priority 1: kaggle.json in current directory\n",
        "    if kaggle_json_path.exists():\n",
        "        print(\"‚úì Found kaggle.json in current directory\")\n",
        "\n",
        "        # Create .kaggle directory if it doesn't exist\n",
        "        kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Copy to ~/.kaggle/kaggle.json\n",
        "        import shutil\n",
        "        shutil.copy(kaggle_json_path, kaggle_config_path)\n",
        "\n",
        "        # Set proper permissions (required by Kaggle API)\n",
        "        kaggle_config_path.chmod(0o600)\n",
        "\n",
        "        print(f\"‚úì Kaggle credentials installed to {kaggle_config_path}\")\n",
        "        return True\n",
        "\n",
        "    # Priority 2: kaggle.json already in ~/.kaggle/\n",
        "    elif kaggle_config_path.exists():\n",
        "        print(f\"‚úì Using existing kaggle.json from {kaggle_config_path}\")\n",
        "        return True\n",
        "\n",
        "    # Priority 3: Use credentials from config\n",
        "    elif config.KAGGLE_USERNAME and config.KAGGLE_KEY:\n",
        "        print(\"‚ÑπÔ∏è  Using credentials from CCROPConfig\")\n",
        "        os.environ['KAGGLE_USERNAME'] = config.KAGGLE_USERNAME\n",
        "        os.environ['KAGGLE_KEY'] = config.KAGGLE_KEY\n",
        "        return True\n",
        "\n",
        "    # No credentials found\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"‚ö†Ô∏è  KAGGLE AUTHENTICATION REQUIRED\")\n",
        "        print(\"=\"*70)\n",
        "        print(\"\\nPlease provide Kaggle credentials using ONE of these methods:\\n\")\n",
        "        print(\"METHOD 1 (Recommended): Upload kaggle.json file\")\n",
        "        print(\"  1. Download kaggle.json from https://www.kaggle.com/settings\")\n",
        "        print(\"  2. Upload it to this Colab notebook using the file browser\")\n",
        "        print(\"  3. Re-run this cell\\n\")\n",
        "        print(\"METHOD 2: Set credentials in CCROPConfig class\")\n",
        "        print(\"  1. Edit the CCROPConfig class above\")\n",
        "        print(\"  2. Set KAGGLE_USERNAME and KAGGLE_KEY\")\n",
        "        print(\"  3. Re-run this cell\\n\")\n",
        "        print(\"=\"*70)\n",
        "        return False\n",
        "\n",
        "# Setup Kaggle authentication\n",
        "if not setup_kaggle_credentials(config):\n",
        "    raise RuntimeError(\"Kaggle authentication failed. Please provide credentials.\")\n",
        "\n",
        "# Verify authentication\n",
        "print(\"\\nVerifying Kaggle API access...\")\n",
        "auth_status = os.system(\"kaggle datasets list -s cannabis > /dev/null 2>&1\")\n",
        "if auth_status != 0:\n",
        "    print(\"‚ùå Kaggle authentication failed\")\n",
        "    print(\"\\nTroubleshooting:\")\n",
        "    print(\"  ‚Ä¢ Ensure kaggle.json is uploaded to the notebook directory\")\n",
        "    print(\"  ‚Ä¢ Check that your Kaggle API token is valid\")\n",
        "    print(\"  ‚Ä¢ Verify credentials at https://www.kaggle.com/settings\")\n",
        "    raise RuntimeError(\"Cannot proceed without valid Kaggle credentials\")\n",
        "else:\n",
        "    print(\"‚úì Kaggle API authenticated successfully\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Enhanced Dataset Handler\n",
        "# ------------------------------\n",
        "class DatasetManager:\n",
        "    \"\"\"Manages dataset download, extraction, and organization\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def download_dataset(self):\n",
        "        \"\"\"Download dataset with fallback support\"\"\"\n",
        "        print(f\"üì• Downloading dataset: {self.config.DATASET_MAIN}\")\n",
        "        status = os.system(f\"kaggle datasets download -d {self.config.DATASET_MAIN} -p {self.config.ROOT_DIR} 2>/dev/null\")\n",
        "\n",
        "        if status != 0:\n",
        "            print(f\"‚ö†Ô∏è  Primary dataset failed. Trying fallback: {self.config.DATASET_FALLBACK}\")\n",
        "            status = os.system(f\"kaggle datasets download -d {self.config.DATASET_FALLBACK} -p {self.config.ROOT_DIR} 2>/dev/null\")\n",
        "\n",
        "        if status != 0:\n",
        "            raise RuntimeError(\"Failed to download dataset. Check Kaggle credentials and dataset availability.\")\n",
        "\n",
        "        print(\"‚úì Dataset downloaded successfully\")\n",
        "\n",
        "    def extract_dataset(self):\n",
        "        \"\"\"Extract ZIP file\"\"\"\n",
        "        zip_files = list(Path(self.config.ROOT_DIR).glob(\"*.zip\"))\n",
        "\n",
        "        if not zip_files:\n",
        "            raise FileNotFoundError(\"No ZIP file found in dataset directory\")\n",
        "\n",
        "        zip_file = zip_files[0]\n",
        "        print(f\"üì¶ Extracting: {zip_file.name}\")\n",
        "\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.config.ROOT_DIR)\n",
        "\n",
        "        print(\"‚úì Dataset extracted successfully\")\n",
        "\n",
        "    def find_dataset_path(self):\n",
        "        \"\"\"Auto-detect dataset folder with improved logic\"\"\"\n",
        "        def has_multiple_class_folders(path):\n",
        "            try:\n",
        "                subdirs = [d for d in os.listdir(path)\n",
        "                          if os.path.isdir(os.path.join(path, d)) and not d.startswith('.')]\n",
        "                return len(subdirs) > 1\n",
        "            except:\n",
        "                return False\n",
        "\n",
        "        # Search for valid dataset folder\n",
        "        for root, dirs, _ in os.walk(self.config.ROOT_DIR):\n",
        "            if has_multiple_class_folders(root):\n",
        "                # Check for 'color' subfolder (Plant Diseases dataset)\n",
        "                color_dir = os.path.join(root, \"color\")\n",
        "                if os.path.exists(color_dir) and has_multiple_class_folders(color_dir):\n",
        "                    return color_dir\n",
        "                return root\n",
        "\n",
        "        raise FileNotFoundError(\"Unable to locate valid dataset folder with multiple class directories\")\n",
        "\n",
        "    def get_class_info(self, dataset_path):\n",
        "        \"\"\"Get class names and create stress mapping\"\"\"\n",
        "        classes = sorted([d for d in os.listdir(dataset_path)\n",
        "                         if os.path.isdir(os.path.join(dataset_path, d)) and not d.startswith('.')])\n",
        "\n",
        "        if len(classes) == 0:\n",
        "            raise ValueError(\"No class folders found in dataset\")\n",
        "\n",
        "        # Create stress mapping (0-100 scale)\n",
        "        if len(classes) == 1:\n",
        "            stress_mapping = {classes[0]: 50.0}\n",
        "        else:\n",
        "            stress_mapping = {cls: idx * 100.0 / (len(classes) - 1)\n",
        "                            for idx, cls in enumerate(classes)}\n",
        "\n",
        "        return classes, stress_mapping\n",
        "\n",
        "# Initialize dataset manager\n",
        "dm = DatasetManager(config)\n",
        "\n",
        "# Download and extract if needed\n",
        "if not any(Path(config.ROOT_DIR).glob(\"*.zip\")) and not list(Path(config.ROOT_DIR).glob(\"*/*\")):\n",
        "    dm.download_dataset()\n",
        "    dm.extract_dataset()\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Dataset already exists, skipping download\")\n",
        "\n",
        "# Find dataset path and get classes\n",
        "DATASET_PATH = dm.find_dataset_path()\n",
        "CLASSES, STRESS_MAPPING = dm.get_class_info(DATASET_PATH)\n",
        "\n",
        "print(f\"‚úì Dataset path: {DATASET_PATH}\")\n",
        "print(f\"‚úì Classes detected: {len(CLASSES)}\")\n",
        "print(f\"  Sample classes: {CLASSES[:5]}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Enhanced Data Augmentation\n",
        "# ------------------------------\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((config.INPUT_SIZE, config.INPUT_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.2),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomResizedCrop(config.INPUT_SIZE, scale=(0.7, 1.0)),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((config.INPUT_SIZE, config.INPUT_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ------------------------------\n",
        "# 7. Dataset Loading with Class Balancing\n",
        "# ------------------------------\n",
        "full_dataset = datasets.ImageFolder(root=DATASET_PATH, transform=train_transform)\n",
        "\n",
        "# Calculate class weights for balanced training\n",
        "class_counts = np.bincount([label for _, label in full_dataset])\n",
        "class_weights = 1.0 / torch.Tensor(class_counts)\n",
        "sample_weights = [class_weights[label] for _, label in full_dataset]\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(config.TRAIN_SPLIT * len(full_dataset))\n",
        "val_size = int(config.VAL_SPLIT * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "# Apply transforms\n",
        "val_dataset.dataset.transform = val_test_transform\n",
        "test_dataset.dataset.transform = val_test_transform\n",
        "\n",
        "# Create weighted sampler for balanced training\n",
        "train_indices = train_dataset.indices\n",
        "train_sample_weights = [sample_weights[i] for i in train_indices]\n",
        "sampler = WeightedRandomSampler(train_sample_weights, len(train_sample_weights))\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE,\n",
        "                         sampler=sampler, num_workers=config.NUM_WORKERS)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE,\n",
        "                       shuffle=False, num_workers=config.NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE,\n",
        "                        shuffle=False, num_workers=config.NUM_WORKERS)\n",
        "\n",
        "print(f\"‚úì Dataset split: Train={train_size}, Val={val_size}, Test={test_size}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 8. Enhanced Model Architecture\n",
        "# ------------------------------\n",
        "class CCROPModel:\n",
        "    \"\"\"Wrapper for CCROP model with flexible architecture\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def create_model(arch, num_classes, pretrained=True):\n",
        "        \"\"\"Create model based on architecture choice\"\"\"\n",
        "        if arch == \"resnet18\":\n",
        "            model = models.resnet18(pretrained=pretrained)\n",
        "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        elif arch == \"resnet34\":\n",
        "            model = models.resnet34(pretrained=pretrained)\n",
        "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        elif arch == \"resnet50\":\n",
        "            model = models.resnet50(pretrained=pretrained)\n",
        "            model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        elif arch == \"efficientnet_b0\":\n",
        "            model = models.efficientnet_b0(pretrained=pretrained)\n",
        "            model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported architecture: {arch}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "model = CCROPModel.create_model(config.MODEL_ARCH, len(CLASSES))\n",
        "\n",
        "# Unfreeze all layers for fine-tuning\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"‚úì Model: {config.MODEL_ARCH} on {device}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# ------------------------------\n",
        "# 9. Training Components\n",
        "# ------------------------------\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE,\n",
        "                       weight_decay=config.WEIGHT_DECAY)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                 factor=0.5, patience=3)\n",
        "\n",
        "# ------------------------------\n",
        "# 10. Enhanced Training Loop with Metrics Tracking\n",
        "# ------------------------------\n",
        "class MetricsTracker:\n",
        "    \"\"\"Track and visualize training metrics\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.history = {\n",
        "            'train_loss': [], 'val_loss': [],\n",
        "            'train_acc': [], 'val_acc': [],\n",
        "            'learning_rates': [], 'epochs': []\n",
        "        }\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.best_epoch = 0\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def update(self, epoch, train_loss, val_loss, train_acc, val_acc, lr):\n",
        "        self.history['epochs'].append(epoch)\n",
        "        self.history['train_loss'].append(train_loss)\n",
        "        self.history['val_loss'].append(val_loss)\n",
        "        self.history['train_acc'].append(train_acc)\n",
        "        self.history['val_acc'].append(val_acc)\n",
        "        self.history['learning_rates'].append(lr)\n",
        "\n",
        "        if val_loss < self.best_val_loss:\n",
        "            self.best_val_loss = val_loss\n",
        "            self.best_epoch = epoch\n",
        "            self.patience_counter = 0\n",
        "            return True\n",
        "        else:\n",
        "            self.patience_counter += 1\n",
        "            return False\n",
        "\n",
        "    def should_stop(self, patience):\n",
        "        return self.patience_counter >= patience\n",
        "\n",
        "    def plot_history(self, save_path=None):\n",
        "        \"\"\"Plot training history\"\"\"\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "        # Loss\n",
        "        axes[0].plot(self.history['epochs'], self.history['train_loss'], label='Train Loss')\n",
        "        axes[0].plot(self.history['epochs'], self.history['val_loss'], label='Val Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].set_title('Training and Validation Loss')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True)\n",
        "\n",
        "        # Accuracy\n",
        "        axes[1].plot(self.history['epochs'], self.history['train_acc'], label='Train Acc')\n",
        "        axes[1].plot(self.history['epochs'], self.history['val_acc'], label='Val Acc')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('Accuracy (%)')\n",
        "        axes[1].set_title('Training and Validation Accuracy')\n",
        "        axes[1].legend()\n",
        "        axes[1].grid(True)\n",
        "\n",
        "        # Learning Rate\n",
        "        axes[2].plot(self.history['epochs'], self.history['learning_rates'])\n",
        "        axes[2].set_xlabel('Epoch')\n",
        "        axes[2].set_ylabel('Learning Rate')\n",
        "        axes[2].set_title('Learning Rate Schedule')\n",
        "        axes[2].set_yscale('log')\n",
        "        axes[2].grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            print(f\"‚úì Training history saved to {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def save_history(self, path):\n",
        "        \"\"\"Save history to JSON\"\"\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.history, f, indent=2)\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Training loop\n",
        "metrics = MetricsTracker()\n",
        "start_time = time.time()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ Starting Training\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Track metrics\n",
        "        is_best = metrics.update(epoch + 1, train_loss, val_loss, train_acc, val_acc, current_lr)\n",
        "\n",
        "        # Print progress\n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(f\"Epoch [{epoch+1}/{config.EPOCHS}] ({epoch_time:.1f}s)\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"  LR: {current_lr:.6f} {'‚úì BEST' if is_best else ''}\")\n",
        "\n",
        "        # Save checkpoints\n",
        "        if is_best:\n",
        "            checkpoint_path = os.path.join(config.CHECKPOINT_DIR, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'val_acc': val_acc,\n",
        "                'classes': CLASSES,\n",
        "                'stress_mapping': STRESS_MAPPING,\n",
        "                'config': vars(config)\n",
        "            }, checkpoint_path)\n",
        "\n",
        "        # Early stopping\n",
        "        if metrics.should_stop(config.EARLY_STOPPING_PATIENCE):\n",
        "            print(f\"\\n‚ö†Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\n‚úì Training completed in {training_time/60:.1f} minutes\")\n",
        "    print(f\"  Best validation loss: {metrics.best_val_loss:.4f} at epoch {metrics.best_epoch}\")\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
        "    checkpoint_path = os.path.join(config.CHECKPOINT_DIR, \"interrupted_model.pth\")\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"‚úì Model saved to {checkpoint_path}\")\n",
        "\n",
        "# Plot and save training history\n",
        "history_plot_path = os.path.join(config.RESULTS_DIR, \"training_history.png\")\n",
        "history_json_path = os.path.join(config.RESULTS_DIR, \"training_history.json\")\n",
        "metrics.plot_history(save_path=history_plot_path)\n",
        "metrics.save_history(history_json_path)\n",
        "\n",
        "# ------------------------------\n",
        "# 11. Comprehensive Evaluation\n",
        "# ------------------------------\n",
        "def evaluate_model(model, loader, device, class_names):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels, all_preds, average='weighted'\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä Evaluation Results\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Overall Accuracy: {accuracy*100:.2f}%\")\n",
        "    print(f\"Weighted Precision: {precision:.4f}\")\n",
        "    print(f\"Weighted Recall: {recall:.4f}\")\n",
        "    print(f\"Weighted F1-Score: {f1:.4f}\")\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    cm_path = os.path.join(config.RESULTS_DIR, \"confusion_matrix.png\")\n",
        "    plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"‚úì Confusion matrix saved to {cm_path}\")\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels,\n",
        "        'probabilities': all_probs\n",
        "    }\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = evaluate_model(model, test_loader, device, CLASSES)\n",
        "\n",
        "# ------------------------------\n",
        "# 12. Stress Prediction System\n",
        "# ------------------------------\n",
        "class StressPredictor:\n",
        "    \"\"\"Enhanced stress prediction with confidence scores\"\"\"\n",
        "\n",
        "    def __init__(self, model, transform, stress_mapping, class_names, device):\n",
        "        self.model = model\n",
        "        self.transform = transform\n",
        "        self.stress_mapping = stress_mapping\n",
        "        self.class_names = sorted(stress_mapping.keys())\n",
        "        self.device = device\n",
        "        self.model.eval()\n",
        "\n",
        "    def predict_from_path(self, img_path):\n",
        "        \"\"\"Predict stress from image path\"\"\"\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Cannot read image: {img_path}\")\n",
        "        return self.predict_from_array(image)\n",
        "\n",
        "    def predict_from_array(self, image):\n",
        "        \"\"\"Predict stress from numpy array (BGR format)\"\"\"\n",
        "        # Convert BGR to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        pil_image = Image.fromarray(image_rgb)\n",
        "\n",
        "        # Transform and predict\n",
        "        img_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(img_tensor)\n",
        "            probs = F.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        # Calculate stress score\n",
        "        scores = np.array([self.stress_mapping[c] for c in self.class_names])\n",
        "        stress_score = np.sum(probs * scores)\n",
        "\n",
        "        # Get top predictions\n",
        "        top_k = 3\n",
        "        top_indices = np.argsort(probs)[::-1][:top_k]\n",
        "        top_classes = [(self.class_names[i], probs[i]*100) for i in top_indices]\n",
        "\n",
        "        return {\n",
        "            'stress_score': stress_score,\n",
        "            'confidence': probs.max() * 100,\n",
        "            'top_predictions': top_classes,\n",
        "            'all_probabilities': dict(zip(self.class_names, probs))\n",
        "        }\n",
        "\n",
        "    def predict_batch(self, image_paths):\n",
        "        \"\"\"Predict stress for multiple images\"\"\"\n",
        "        results = []\n",
        "        for path in image_paths:\n",
        "            try:\n",
        "                result = self.predict_from_path(path)\n",
        "                result['image_path'] = path\n",
        "                results.append(result)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {path}: {e}\")\n",
        "        return results\n",
        "\n",
        "# Initialize predictor\n",
        "predictor = StressPredictor(model, val_test_transform, STRESS_MAPPING, CLASSES, device)\n",
        "\n",
        "print(\"\\n‚úì Stress predictor initialized and ready\")\n",
        "\n",
        "# ------------------------------\n",
        "# 13. Real-time Webcam Stress Detection\n",
        "# ------------------------------\n",
        "def webcam_stress_monitor(predictor, display_size=(640, 480)):\n",
        "    \"\"\"Real-time webcam monitoring with enhanced visualization\"\"\"\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"‚ùå Cannot open webcam\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nüé• Starting webcam monitoring\")\n",
        "    print(\"Press 'q' to quit, 's' to save snapshot\")\n",
        "\n",
        "    snapshot_count = 0\n",
        "    fps_history = []\n",
        "\n",
        "    while True:\n",
        "        start_time = time.time()\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            print(\"‚ùå Failed to grab frame\")\n",
        "            break\n",
        "\n",
        "        # Resize for display\n",
        "        display_frame = cv2.resize(frame, display_size)\n",
        "\n",
        "        # Predict stress\n",
        "        try:\n",
        "            result = predictor.predict_from_array(frame)\n",
        "            stress_score = result['stress_score']\n",
        "            confidence = result['confidence']\n",
        "            top_class, top_prob = result['top_predictions'][0]\n",
        "\n",
        "            # Determine color based on stress level\n",
        "            if stress_score < 33:\n",
        "                color = (0, 255, 0)  # Green - healthy\n",
        "                status = \"HEALTHY\"\n",
        "            elif stress_score < 66:\n",
        "                color = (0, 165, 255)  # Orange - moderate\n",
        "                status = \"MODERATE\"\n",
        "            else:\n",
        "                color = (0, 0, 255)  # Red - severe\n",
        "                status = \"SEVERE\"\n",
        "\n",
        "            # Draw info panel\n",
        "            panel_height = 120\n",
        "            panel = np.zeros((panel_height, display_size[0], 3), dtype=np.uint8)\n",
        "\n",
        "            # Add text\n",
        "            cv2.putText(panel, f\"Status: {status}\", (10, 25),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
        "            cv2.putText(panel, f\"Stress Score: {stress_score:.1f}%\", (10, 55),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "            cv2.putText(panel, f\"Confidence: {confidence:.1f}%\", (10, 80),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "            cv2.putText(panel, f\"Top Class: {top_class[:30]}\", (10, 105),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)\n",
        "\n",
        "            # Calculate FPS\n",
        "            fps = 1.0 / (time.time() - start_time)\n",
        "            fps_history.append(fps)\n",
        "            if len(fps_history) > 30:\n",
        "                fps_history.pop(0)\n",
        "            avg_fps = np.mean(fps_history)\n",
        "\n",
        "            cv2.putText(panel, f\"FPS: {avg_fps:.1f}\", (display_size[0]-120, 25),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "\n",
        "            # Draw stress bar\n",
        "            bar_width = int((stress_score / 100) * (display_size[0] - 20))\n",
        "            cv2.rectangle(panel, (10, panel_height-20),\n",
        "                         (10 + bar_width, panel_height-10), color, -1)\n",
        "            cv2.rectangle(panel, (10, panel_height-20),\n",
        "                         (display_size[0]-10, panel_height-10), (100, 100, 100), 2)\n",
        "\n",
        "            # Combine frame and panel\n",
        "            combined = np.vstack([display_frame, panel])\n",
        "\n",
        "        except Exception as e:\n",
        "            combined = display_frame\n",
        "            cv2.putText(combined, f\"Error: {str(e)[:50]}\", (10, 30),\n",
        "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "        cv2.imshow(\"CCROP - Leaf Stress Monitor\", combined)\n",
        "\n",
        "        key = cv2.waitKey(1) & 0xFF\n",
        "        if key == ord('q'):\n",
        "            break\n",
        "        elif key == ord('s'):\n",
        "            snapshot_path = os.path.join(config.RESULTS_DIR, f\"snapshot_{snapshot_count:03d}.jpg\")\n",
        "            cv2.imwrite(snapshot_path, frame)\n",
        "            print(f\"‚úì Snapshot saved: {snapshot_path}\")\n",
        "            snapshot_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    print(\"\\n‚úì Webcam monitoring stopped\")\n",
        "\n",
        "# ------------------------------\n",
        "# 14. Batch Inference on Directory\n",
        "# ------------------------------\n",
        "def batch_inference(predictor, image_dir, output_csv=None):\n",
        "    \"\"\"Run inference on all images in a directory\"\"\"\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
        "    image_paths = []\n",
        "\n",
        "    for ext in image_extensions:\n",
        "        image_paths.extend(Path(image_dir).glob(f\"*{ext}\"))\n",
        "        image_paths.extend(Path(image_dir).glob(f\"*{ext.upper()}\"))\n",
        "\n",
        "    if not image_paths:\n",
        "        print(f\"‚ùå No images found in {image_dir}\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüì∏ Processing {len(image_paths)} images...\")\n",
        "\n",
        "    results = []\n",
        "    for i, img_path in enumerate(image_paths):\n",
        "        try:\n",
        "            result = predictor.predict_from_path(str(img_path))\n",
        "            result['filename'] = img_path.name\n",
        "            result['filepath'] = str(img_path)\n",
        "            results.append(result)\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"  Processed {i+1}/{len(image_paths)} images\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error processing {img_path.name}: {e}\")\n",
        "\n",
        "    # Create DataFrame\n",
        "    df_results = pd.DataFrame([{\n",
        "        'filename': r['filename'],\n",
        "        'stress_score': r['stress_score'],\n",
        "        'confidence': r['confidence'],\n",
        "        'top_class': r['top_predictions'][0][0],\n",
        "        'top_class_prob': r['top_predictions'][0][1]\n",
        "    } for r in results])\n",
        "\n",
        "    # Save to CSV if requested\n",
        "    if output_csv:\n",
        "        df_results.to_csv(output_csv, index=False)\n",
        "        print(f\"\\n‚úì Results saved to {output_csv}\")\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä Batch Inference Summary\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total images processed: {len(results)}\")\n",
        "    print(f\"Average stress score: {df_results['stress_score'].mean():.2f}%\")\n",
        "    print(f\"Std deviation: {df_results['stress_score'].std():.2f}%\")\n",
        "    print(f\"Min stress: {df_results['stress_score'].min():.2f}%\")\n",
        "    print(f\"Max stress: {df_results['stress_score'].max():.2f}%\")\n",
        "    print(f\"Average confidence: {df_results['confidence'].mean():.2f}%\")\n",
        "\n",
        "    # Stress distribution\n",
        "    healthy = len(df_results[df_results['stress_score'] < 33])\n",
        "    moderate = len(df_results[(df_results['stress_score'] >= 33) & (df_results['stress_score'] < 66)])\n",
        "    severe = len(df_results[df_results['stress_score'] >= 66])\n",
        "\n",
        "    print(f\"\\nStress Distribution:\")\n",
        "    print(f\"  Healthy (<33%): {healthy} ({healthy/len(results)*100:.1f}%)\")\n",
        "    print(f\"  Moderate (33-66%): {moderate} ({moderate/len(results)*100:.1f}%)\")\n",
        "    print(f\"  Severe (>66%): {severe} ({severe/len(results)*100:.1f}%)\")\n",
        "\n",
        "    return df_results\n",
        "\n",
        "# ------------------------------\n",
        "# 15. Visualization Tools\n",
        "# ------------------------------\n",
        "def visualize_predictions(predictor, image_paths, save_path=None):\n",
        "    \"\"\"Visualize predictions for multiple images\"\"\"\n",
        "    n_images = len(image_paths)\n",
        "    cols = min(4, n_images)\n",
        "    rows = (n_images + cols - 1) // cols\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
        "    if n_images == 1:\n",
        "        axes = [axes]\n",
        "    else:\n",
        "        axes = axes.flatten()\n",
        "\n",
        "    for idx, img_path in enumerate(image_paths):\n",
        "        if idx >= len(axes):\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            # Load and predict\n",
        "            image = cv2.imread(str(img_path))\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            result = predictor.predict_from_array(image)\n",
        "\n",
        "            # Display\n",
        "            axes[idx].imshow(image_rgb)\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "            # Create title with results\n",
        "            stress = result['stress_score']\n",
        "            conf = result['confidence']\n",
        "            top_class = result['top_predictions'][0][0]\n",
        "\n",
        "            if stress < 33:\n",
        "                color = 'green'\n",
        "                status = 'Healthy'\n",
        "            elif stress < 66:\n",
        "                color = 'orange'\n",
        "                status = 'Moderate'\n",
        "            else:\n",
        "                color = 'red'\n",
        "                status = 'Severe'\n",
        "\n",
        "            title = f\"{status}\\nStress: {stress:.1f}% | Conf: {conf:.1f}%\\n{top_class[:20]}\"\n",
        "            axes[idx].set_title(title, fontsize=9, color=color, weight='bold')\n",
        "\n",
        "        except Exception as e:\n",
        "            axes[idx].text(0.5, 0.5, f\"Error:\\n{str(e)[:30]}\",\n",
        "                          ha='center', va='center', transform=axes[idx].transAxes)\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "    # Hide extra subplots\n",
        "    for idx in range(n_images, len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"‚úì Visualization saved to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_stress_distribution(df_results, save_path=None):\n",
        "    \"\"\"Plot stress score distribution\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Histogram\n",
        "    axes[0].hist(df_results['stress_score'], bins=20, color='skyblue', edgecolor='black')\n",
        "    axes[0].axvline(df_results['stress_score'].mean(), color='red',\n",
        "                    linestyle='--', label=f\"Mean: {df_results['stress_score'].mean():.1f}%\")\n",
        "    axes[0].set_xlabel('Stress Score (%)')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('Stress Score Distribution')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Box plot by category\n",
        "    categories = []\n",
        "    for score in df_results['stress_score']:\n",
        "        if score < 33:\n",
        "            categories.append('Healthy')\n",
        "        elif score < 66:\n",
        "            categories.append('Moderate')\n",
        "        else:\n",
        "            categories.append('Severe')\n",
        "\n",
        "    df_results['category'] = categories\n",
        "\n",
        "    category_order = ['Healthy', 'Moderate', 'Severe']\n",
        "    colors = ['green', 'orange', 'red']\n",
        "\n",
        "    box_data = [df_results[df_results['category'] == cat]['stress_score'].values\n",
        "                for cat in category_order]\n",
        "\n",
        "    bp = axes[1].boxplot(box_data, labels=category_order, patch_artist=True)\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.6)\n",
        "\n",
        "    axes[1].set_ylabel('Stress Score (%)')\n",
        "    axes[1].set_title('Stress Scores by Category')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        print(f\"‚úì Distribution plot saved to {save_path}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ------------------------------\n",
        "# 16. Model Export and Deployment\n",
        "# ------------------------------\n",
        "def export_model_for_deployment(model, save_dir, example_input_size=(1, 3, 224, 224)):\n",
        "    \"\"\"Export model in multiple formats for deployment\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # 1. PyTorch model (.pth)\n",
        "    torch_path = os.path.join(save_dir, \"ccrop_model.pth\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'classes': CLASSES,\n",
        "        'stress_mapping': STRESS_MAPPING,\n",
        "        'config': vars(config),\n",
        "        'architecture': config.MODEL_ARCH\n",
        "    }, torch_path)\n",
        "    print(f\"‚úì PyTorch model saved: {torch_path}\")\n",
        "\n",
        "    # 2. TorchScript (for production deployment)\n",
        "    example_input = torch.randn(example_input_size).to(device)\n",
        "    traced_model = torch.jit.trace(model, example_input)\n",
        "    torchscript_path = os.path.join(save_dir, \"ccrop_model_scripted.pt\")\n",
        "    traced_model.save(torchscript_path)\n",
        "    print(f\"‚úì TorchScript model saved: {torchscript_path}\")\n",
        "\n",
        "    # 3. ONNX (for cross-platform deployment)\n",
        "    try:\n",
        "        onnx_path = os.path.join(save_dir, \"ccrop_model.onnx\")\n",
        "        torch.onnx.export(\n",
        "            model,\n",
        "            example_input,\n",
        "            onnx_path,\n",
        "            export_params=True,\n",
        "            opset_version=11,\n",
        "            do_constant_folding=True,\n",
        "            input_names=['input'],\n",
        "            output_names=['output'],\n",
        "            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "        )\n",
        "        print(f\"‚úì ONNX model saved: {onnx_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  ONNX export failed: {e}\")\n",
        "\n",
        "    # 4. Save metadata\n",
        "    metadata = {\n",
        "        'model_architecture': config.MODEL_ARCH,\n",
        "        'num_classes': len(CLASSES),\n",
        "        'classes': CLASSES,\n",
        "        'stress_mapping': STRESS_MAPPING,\n",
        "        'input_size': config.INPUT_SIZE,\n",
        "        'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'best_val_accuracy': test_results['accuracy'] * 100,\n",
        "        'normalization_mean': [0.485, 0.456, 0.406],\n",
        "        'normalization_std': [0.229, 0.224, 0.225]\n",
        "    }\n",
        "\n",
        "    metadata_path = os.path.join(save_dir, \"model_metadata.json\")\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"‚úì Metadata saved: {metadata_path}\")\n",
        "\n",
        "    print(f\"\\n‚úì All deployment files saved to: {save_dir}\")\n",
        "\n",
        "# Export model for deployment\n",
        "deployment_dir = os.path.join(config.RESULTS_DIR, \"deployment\")\n",
        "export_model_for_deployment(model, deployment_dir)\n",
        "\n",
        "# ------------------------------\n",
        "# 17. Interactive Usage Examples\n",
        "# ------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéØ CCROP System Ready - Usage Examples\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\"\"\n",
        "# Example 1: Predict stress for a single image\n",
        "result = predictor.predict_from_path('path/to/leaf_image.jpg')\n",
        "print(f\"Stress Score: {result['stress_score']:.1f}%\")\n",
        "print(f\"Top Prediction: {result['top_predictions'][0]}\")\n",
        "\n",
        "# Example 2: Start webcam monitoring\n",
        "webcam_stress_monitor(predictor)\n",
        "\n",
        "# Example 3: Batch process a directory\n",
        "df = batch_inference(predictor, 'path/to/image_folder/',\n",
        "                     output_csv='results.csv')\n",
        "\n",
        "# Example 4: Visualize predictions\n",
        "image_list = ['img1.jpg', 'img2.jpg', 'img3.jpg']\n",
        "visualize_predictions(predictor, image_list,\n",
        "                     save_path='predictions.png')\n",
        "\n",
        "# Example 5: Plot stress distribution from batch results\n",
        "plot_stress_distribution(df, save_path='distribution.png')\n",
        "\n",
        "# Example 6: Load model for inference only\n",
        "checkpoint = torch.load('checkpoints/best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "predictor = StressPredictor(model, val_test_transform,\n",
        "                           STRESS_MAPPING, CLASSES, device)\n",
        "\"\"\")\n",
        "\n",
        "# ------------------------------\n",
        "# 18. Generate Summary Report\n",
        "# ------------------------------\n",
        "def generate_summary_report():\n",
        "    \"\"\"Generate comprehensive summary report\"\"\"\n",
        "    report_path = os.path.join(config.RESULTS_DIR, \"training_report.txt\")\n",
        "\n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(\"=\"*70 + \"\\n\")\n",
        "        f.write(\"CCROP CANNABIS LEAF DISEASE AI - TRAINING REPORT\\n\")\n",
        "        f.write(\"=\"*70 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Device: {device}\\n\\n\")\n",
        "\n",
        "        f.write(\"MODEL CONFIGURATION\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        f.write(f\"Architecture: {config.MODEL_ARCH}\\n\")\n",
        "        f.write(f\"Input Size: {config.INPUT_SIZE}x{config.INPUT_SIZE}\\n\")\n",
        "        f.write(f\"Number of Classes: {len(CLASSES)}\\n\")\n",
        "        f.write(f\"Batch Size: {config.BATCH_SIZE}\\n\")\n",
        "        f.write(f\"Learning Rate: {config.LEARNING_RATE}\\n\")\n",
        "        f.write(f\"Epochs Trained: {len(metrics.history['epochs'])}\\n\\n\")\n",
        "\n",
        "        f.write(\"DATASET INFORMATION\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        f.write(f\"Dataset Path: {DATASET_PATH}\\n\")\n",
        "        f.write(f\"Total Samples: {len(full_dataset)}\\n\")\n",
        "        f.write(f\"Training Samples: {train_size}\\n\")\n",
        "        f.write(f\"Validation Samples: {val_size}\\n\")\n",
        "        f.write(f\"Test Samples: {test_size}\\n\\n\")\n",
        "\n",
        "        f.write(\"CLASSES\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        for i, cls in enumerate(CLASSES[:10]):\n",
        "            f.write(f\"{i+1}. {cls}\\n\")\n",
        "        if len(CLASSES) > 10:\n",
        "            f.write(f\"... and {len(CLASSES)-10} more\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "        f.write(\"TRAINING RESULTS\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        f.write(f\"Best Validation Loss: {metrics.best_val_loss:.4f}\\n\")\n",
        "        f.write(f\"Best Epoch: {metrics.best_epoch}\\n\")\n",
        "        f.write(f\"Final Train Loss: {metrics.history['train_loss'][-1]:.4f}\\n\")\n",
        "        f.write(f\"Final Val Loss: {metrics.history['val_loss'][-1]:.4f}\\n\")\n",
        "        f.write(f\"Final Train Accuracy: {metrics.history['train_acc'][-1]:.2f}%\\n\")\n",
        "        f.write(f\"Final Val Accuracy: {metrics.history['val_acc'][-1]:.2f}%\\n\\n\")\n",
        "\n",
        "        f.write(\"TEST SET EVALUATION\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        f.write(f\"Test Accuracy: {test_results['accuracy']*100:.2f}%\\n\")\n",
        "        f.write(f\"Test Precision: {test_results['precision']:.4f}\\n\")\n",
        "        f.write(f\"Test Recall: {test_results['recall']:.4f}\\n\")\n",
        "        f.write(f\"Test F1-Score: {test_results['f1']:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"FILES GENERATED\\n\")\n",
        "        f.write(\"-\"*70 + \"\\n\")\n",
        "        f.write(f\"- Best Model: checkpoints/best_model.pth\\n\")\n",
        "        f.write(f\"- Training History: {history_json_path}\\n\")\n",
        "        f.write(f\"- Training Plot: {history_plot_path}\\n\")\n",
        "        f.write(f\"- Confusion Matrix: {config.RESULTS_DIR}/confusion_matrix.png\\n\")\n",
        "        f.write(f\"- Deployment Models: {deployment_dir}/\\n\")\n",
        "        f.write(f\"- This Report: {report_path}\\n\\n\")\n",
        "\n",
        "        f.write(\"=\"*70 + \"\\n\")\n",
        "        f.write(\"Report generated successfully!\\n\")\n",
        "        f.write(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    print(f\"\\n‚úì Summary report saved to {report_path}\")\n",
        "\n",
        "    # Also print to console\n",
        "    with open(report_path, 'r') as f:\n",
        "        print(\"\\n\" + f.read())\n",
        "\n",
        "generate_summary_report()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ CCROP Training Pipeline Completed Successfully!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nNext Steps:\")\n",
        "print(\"1. Review training metrics and confusion matrix\")\n",
        "print(\"2. Test predictions on new images\")\n",
        "print(\"3. Start webcam monitoring: webcam_stress_monitor(predictor)\")\n",
        "print(\"4. Process image batches: batch_inference(predictor, 'folder_path')\")\n",
        "print(\"5. Deploy model using files in:\", deployment_dir)\n",
        "print(\"\\nAll results saved in:\", config.RESULTS_DIR)\n",
        "\n"
      ]
    }
  ]
}